{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tempo.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aliakbarbadri/persian-poetry-creator/blob/master/word-lstm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nF-7m57sBgdp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional, GRU\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras import regularizers\n",
        "import tensorflow.keras.utils as ku \n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Urefe0BUJ5vy",
        "colab_type": "text"
      },
      "source": [
        "usefull links:\n",
        "\n",
        "https://www.tensorflow.org/tutorials/text/text_generation\n",
        "\n",
        "https://github.com/petrosDemetrakopoulos/RNN-Beatles-lyrics-generator/blob/master/model.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pxSaPGaqHfMV",
        "colab_type": "text"
      },
      "source": [
        "# Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NNCS1WcxmTMB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c25d9513-78b6-4597-b22d-e5190e249d40"
      },
      "source": [
        "url = \"https://raw.githubusercontent.com/aliakbarbadri/persian-poetry-creator/master/shahname2.txt\"\n",
        "filepath = keras.utils.get_file(\"shahname2.txt\", url)\n",
        "\n",
        "text = open(filepath, 'rb').read().decode(encoding='utf-8')\n",
        "text = text.replace(\"\\t\",\" \\t \").replace(\"\\n\", \" \\n \")\n",
        "corpusList = [w for w in text.split(' ')] \n",
        "corpus_words = [i for i in corpusList if i]\n",
        "# corpus_words = corpusToList(text) \n",
        "map(str.strip, corpus_words)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<map at 0x7f586ba7e400>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f11TwQxzmVBv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "445daea5-2d64-4ec7-f093-5efda9c14003"
      },
      "source": [
        "vocab = sorted(set(corpus_words))\n",
        "print('Corpus length (in words):', len(corpus_words))\n",
        "print('Unique words in corpus: {}'.format(len(vocab)))\n",
        "word2idx = {u: i for i, u in enumerate(vocab)}\n",
        "idx2words = np.array(vocab)\n",
        "word_as_int = np.array([word2idx[c] for c in corpus_words])"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Corpus length (in words): 661330\n",
            "Unique words in corpus: 19780\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LBLvTwoNhep0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "seqLength = 20\n",
        "examples_per_epoch = len(corpus_words)//(seqLength + 1)\n",
        "wordDataset = tf.data.Dataset.from_tensor_slices(word_as_int)\n",
        "sequencesOfWords = wordDataset.batch(seqLength + 1, drop_remainder=True)\n",
        "def split_input_target(chunk):\n",
        "  input_text = chunk[:-1]\n",
        "  target_text = chunk[1:]\n",
        "  return input_text, target_text\n",
        "\n",
        "dataset = sequencesOfWords.map(split_input_target)\n",
        "BATCH_SIZE = 64\n",
        "BUFFER_SIZE = 100\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tmgeH1iaHV3D",
        "colab_type": "text"
      },
      "source": [
        "# Model (GRU)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rle0H24_GGoc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_model_gru(vocab_size, embedding_dim, rnn_units, batch_size):\n",
        "  model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
        "                              batch_input_shape=[batch_size, None]),\n",
        "    tf.keras.layers.GRU(rnn_units,\n",
        "                        return_sequences=True,\n",
        "                        stateful=True,\n",
        "                        recurrent_initializer='glorot_uniform'),\n",
        "    tf.keras.layers.Dense(vocab_size)\n",
        "  ])\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ypzdXYqhsqF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "6c1c9515-8721-4e90-f3cd-8ef950cd4635"
      },
      "source": [
        "vocab_size = len(vocab)\n",
        "embedding_dim = 256\n",
        "rnn_units = 1024\n",
        "\n",
        "gru_model = create_model_gru(vocab_size = len(vocab), embedding_dim=embedding_dim, rnn_units=rnn_units, batch_size=BATCH_SIZE)\n",
        "\n",
        "gru_model.summary()\n",
        "\n",
        "gru_model.compile(optimizer='adam', loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])\n",
        "history = gru_model.fit(dataset, epochs=50)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (64, None, 256)           5063680   \n",
            "_________________________________________________________________\n",
            "gru (GRU)                    (64, None, 1024)          3938304   \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (64, None, 19780)         20274500  \n",
            "=================================================================\n",
            "Total params: 29,276,484\n",
            "Trainable params: 29,276,484\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/50\n",
            "492/492 [==============================] - 53s 108ms/step - loss: 6.4114 - accuracy: 0.1199\n",
            "Epoch 2/50\n",
            "492/492 [==============================] - 53s 108ms/step - loss: 5.3184 - accuracy: 0.2062\n",
            "Epoch 3/50\n",
            "492/492 [==============================] - 53s 109ms/step - loss: 4.7593 - accuracy: 0.2485\n",
            "Epoch 4/50\n",
            "492/492 [==============================] - 53s 109ms/step - loss: 4.3526 - accuracy: 0.2705\n",
            "Epoch 5/50\n",
            "492/492 [==============================] - 54s 109ms/step - loss: 3.9885 - accuracy: 0.2952\n",
            "Epoch 6/50\n",
            "492/492 [==============================] - 54s 110ms/step - loss: 3.6552 - accuracy: 0.3278\n",
            "Epoch 7/50\n",
            "492/492 [==============================] - 54s 109ms/step - loss: 3.3648 - accuracy: 0.3623\n",
            "Epoch 8/50\n",
            "492/492 [==============================] - 53s 108ms/step - loss: 3.1193 - accuracy: 0.3959\n",
            "Epoch 9/50\n",
            "492/492 [==============================] - 53s 109ms/step - loss: 2.9097 - accuracy: 0.4268\n",
            "Epoch 10/50\n",
            "492/492 [==============================] - 53s 108ms/step - loss: 2.7316 - accuracy: 0.4560\n",
            "Epoch 11/50\n",
            "492/492 [==============================] - 53s 109ms/step - loss: 2.5783 - accuracy: 0.4812\n",
            "Epoch 12/50\n",
            "492/492 [==============================] - 54s 109ms/step - loss: 2.4485 - accuracy: 0.5046\n",
            "Epoch 13/50\n",
            "492/492 [==============================] - 53s 109ms/step - loss: 2.3359 - accuracy: 0.5248\n",
            "Epoch 14/50\n",
            "492/492 [==============================] - 54s 109ms/step - loss: 2.2357 - accuracy: 0.5437\n",
            "Epoch 15/50\n",
            "492/492 [==============================] - 54s 109ms/step - loss: 2.1511 - accuracy: 0.5601\n",
            "Epoch 16/50\n",
            "492/492 [==============================] - 53s 108ms/step - loss: 2.0724 - accuracy: 0.5756\n",
            "Epoch 17/50\n",
            "492/492 [==============================] - 54s 109ms/step - loss: 2.0054 - accuracy: 0.5886\n",
            "Epoch 18/50\n",
            "492/492 [==============================] - 54s 109ms/step - loss: 1.9413 - accuracy: 0.6021\n",
            "Epoch 19/50\n",
            "492/492 [==============================] - 53s 108ms/step - loss: 1.8833 - accuracy: 0.6136\n",
            "Epoch 20/50\n",
            "492/492 [==============================] - 53s 107ms/step - loss: 1.8312 - accuracy: 0.6242\n",
            "Epoch 21/50\n",
            "492/492 [==============================] - 53s 108ms/step - loss: 1.7863 - accuracy: 0.6341\n",
            "Epoch 22/50\n",
            "492/492 [==============================] - 53s 107ms/step - loss: 1.7465 - accuracy: 0.6424\n",
            "Epoch 23/50\n",
            "492/492 [==============================] - 52s 107ms/step - loss: 1.7041 - accuracy: 0.6516\n",
            "Epoch 24/50\n",
            "492/492 [==============================] - 53s 107ms/step - loss: 1.6721 - accuracy: 0.6581\n",
            "Epoch 25/50\n",
            "492/492 [==============================] - 52s 107ms/step - loss: 1.6373 - accuracy: 0.6661\n",
            "Epoch 26/50\n",
            "492/492 [==============================] - 53s 107ms/step - loss: 1.6094 - accuracy: 0.6721\n",
            "Epoch 27/50\n",
            "492/492 [==============================] - 52s 107ms/step - loss: 1.5812 - accuracy: 0.6778\n",
            "Epoch 28/50\n",
            "492/492 [==============================] - 52s 107ms/step - loss: 1.5545 - accuracy: 0.6843\n",
            "Epoch 29/50\n",
            "492/492 [==============================] - 52s 106ms/step - loss: 1.5318 - accuracy: 0.6890\n",
            "Epoch 30/50\n",
            "492/492 [==============================] - 53s 107ms/step - loss: 1.5079 - accuracy: 0.6942\n",
            "Epoch 31/50\n",
            "492/492 [==============================] - 52s 106ms/step - loss: 1.4886 - accuracy: 0.6983\n",
            "Epoch 32/50\n",
            "492/492 [==============================] - 52s 106ms/step - loss: 1.4701 - accuracy: 0.7022\n",
            "Epoch 33/50\n",
            "492/492 [==============================] - 52s 106ms/step - loss: 1.4546 - accuracy: 0.7057\n",
            "Epoch 34/50\n",
            "492/492 [==============================] - 52s 106ms/step - loss: 1.4360 - accuracy: 0.7093\n",
            "Epoch 35/50\n",
            "492/492 [==============================] - 52s 106ms/step - loss: 1.4201 - accuracy: 0.7128\n",
            "Epoch 36/50\n",
            "492/492 [==============================] - 53s 107ms/step - loss: 1.4053 - accuracy: 0.7168\n",
            "Epoch 37/50\n",
            "492/492 [==============================] - 52s 107ms/step - loss: 1.3977 - accuracy: 0.7184\n",
            "Epoch 38/50\n",
            "492/492 [==============================] - 53s 107ms/step - loss: 1.3829 - accuracy: 0.7211\n",
            "Epoch 39/50\n",
            "492/492 [==============================] - 53s 108ms/step - loss: 1.3709 - accuracy: 0.7244\n",
            "Epoch 40/50\n",
            "492/492 [==============================] - 53s 107ms/step - loss: 1.3613 - accuracy: 0.7260\n",
            "Epoch 41/50\n",
            "492/492 [==============================] - 53s 108ms/step - loss: 1.3561 - accuracy: 0.7263\n",
            "Epoch 42/50\n",
            "492/492 [==============================] - 53s 108ms/step - loss: 1.3445 - accuracy: 0.7293\n",
            "Epoch 43/50\n",
            "492/492 [==============================] - 53s 108ms/step - loss: 1.3346 - accuracy: 0.7318\n",
            "Epoch 44/50\n",
            "492/492 [==============================] - 53s 108ms/step - loss: 1.3261 - accuracy: 0.7328\n",
            "Epoch 45/50\n",
            "492/492 [==============================] - 53s 108ms/step - loss: 1.3217 - accuracy: 0.7338\n",
            "Epoch 46/50\n",
            "492/492 [==============================] - 53s 108ms/step - loss: 1.3099 - accuracy: 0.7366\n",
            "Epoch 47/50\n",
            "492/492 [==============================] - 53s 109ms/step - loss: 1.3076 - accuracy: 0.7370\n",
            "Epoch 48/50\n",
            "492/492 [==============================] - 53s 108ms/step - loss: 1.2980 - accuracy: 0.7390\n",
            "Epoch 49/50\n",
            "492/492 [==============================] - 53s 108ms/step - loss: 1.2930 - accuracy: 0.7394\n",
            "Epoch 50/50\n",
            "492/492 [==============================] - 53s 108ms/step - loss: 1.2867 - accuracy: 0.7411\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hxs6LcdkA-qC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "f3928fb4-4752-42e8-d0e5-bcdb46a78322"
      },
      "source": [
        "main_gru_model = create_model_gru(vocab_size = len(vocab), embedding_dim=embedding_dim, rnn_units=rnn_units, batch_size=1)\n",
        "main_gru_model.set_weights(gru_model.get_weights())\n",
        "main_gru_model.summary()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (1, None, 256)            5063680   \n",
            "_________________________________________________________________\n",
            "gru_1 (GRU)                  (1, None, 1024)           3938304   \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (1, None, 19780)          20274500  \n",
            "=================================================================\n",
            "Total params: 29,276,484\n",
            "Trainable params: 29,276,484\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KL4gEU440DyQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_text(model, start_string, temperature = 1.0):\n",
        "  num_generate = 200\n",
        "  start_string_list =  [w for w in start_string.split(' ')]\n",
        "  input_eval = [word2idx[s] for s in start_string_list]\n",
        "  input_eval = tf.expand_dims(input_eval, 0)\n",
        "  text_generated = []\n",
        "  model.reset_states()\n",
        "  for i in range(num_generate):\n",
        "      predictions = model(input_eval)\n",
        "      predictions = tf.squeeze(predictions, 0)\n",
        "      predictions = predictions / temperature\n",
        "      predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
        "      input_eval = tf.expand_dims([predicted_id], 0)\n",
        "      text_generated.append(idx2words[predicted_id])\n",
        "  return (start_string + ' '.join(text_generated))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iJT7mAyIqFoJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# main_gru_model = tf.keras.models.load_model('word_gru.h5')\n",
        "# main_gru_model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yAduGkXN0SYs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "ece11062-442a-4b13-af1b-09e6b33794d7"
      },
      "source": [
        "print(generate_text(main_gru_model, start_string=u\"که ایران چوباغیست خرم بهار\", temperature=1))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "که ایران چوباغیست خرم بهار\t همیشه پرستندهٔ و چهر شاهش دلارای بود \n",
            " به تخت از بدرهای درم داد واسب ورهی \n",
            " یکی تاج پرگوهر شاهوار \t ابا یاره و طوق و زرینه کفش \n",
            " چو از پیش گودرز برخاستند \t بفرمود تا خلعت آراستند \n",
            " از خانهٔ بزم و آز و نیاز \t پس او را همی فر یزدان کنیم \n",
            " بدان نامداران گشته اسفندیار \t که او پیشدستی نماید بکار \n",
            " که از من بجنگ آمدی \n",
            " من از پس گستهم بد کند رای \t که ما هم سرای سپنج \t گه آمد که پیچان تو اندر آریمشان \n",
            " چو این کرده از چرخ گم شد نخست آفرین \t ندانم که دستور باشد مرا نیکخواه \t شوی شادمان روز من بگذرد \n",
            " مرا از غم خویش دریا کند \t برو بر دست تاریک پیچان کنم \n",
            " دل چین چو درد برادر شد او برادر فروشد به خاک \t کجا جای گیرد به دل یاد کرد \n",
            " خردمند را چون نماید برو \t همه پاک پشت او افسر ماه گفت \t چنان هم که با من شب نیزه باید نهاد \t سوی ایران کشید \t گرفت آن سخن پیش او با سپاه \n",
            " بدو گفت سوی کنابد بجنگ \t برین داستان دلیران جنگی\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gEtcew15KI0O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "main_gru_model.save(\"word_gru.h5\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OLOB0rSQHbe7",
        "colab_type": "text"
      },
      "source": [
        "# Model (LSTM)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "47qIyVAcHdak",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_model_lstm(vocab_size, embedding_dim, rnn_units, batch_size):\n",
        "  model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
        "                              batch_input_shape=[batch_size, None]),\n",
        "    tf.keras.layers.LSTM(rnn_units,\n",
        "                        return_sequences=True,\n",
        "                        stateful=True,\n",
        "                        recurrent_initializer='glorot_uniform'),\n",
        "    tf.keras.layers.Dense(vocab_size)\n",
        "  ])\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PLuzYVgrIyLv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "882119b5-8002-4a9e-f0d0-196f1d6a6a63"
      },
      "source": [
        "vocab_size = len(vocab)\n",
        "embedding_dim = 256\n",
        "rnn_units = 1024\n",
        "\n",
        "lstm_model = create_model_lstm(vocab_size = len(vocab), embedding_dim=embedding_dim, rnn_units=rnn_units, batch_size=BATCH_SIZE)\n",
        "lstm_model.summary()\n",
        "lstm_model.compile(optimizer='adam', loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])\n",
        "history = lstm_model.fit(dataset, epochs=50)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_2 (Embedding)      (64, None, 256)           5063680   \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (64, None, 1024)          5246976   \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (64, None, 19780)         20274500  \n",
            "=================================================================\n",
            "Total params: 30,585,156\n",
            "Trainable params: 30,585,156\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/50\n",
            "492/492 [==============================] - 53s 109ms/step - loss: 6.1890 - accuracy: 0.1117\n",
            "Epoch 2/50\n",
            "492/492 [==============================] - 53s 107ms/step - loss: 5.4264 - accuracy: 0.1855\n",
            "Epoch 3/50\n",
            "492/492 [==============================] - 53s 107ms/step - loss: 5.0092 - accuracy: 0.2268\n",
            "Epoch 4/50\n",
            "492/492 [==============================] - 53s 107ms/step - loss: 4.7140 - accuracy: 0.2445\n",
            "Epoch 5/50\n",
            "492/492 [==============================] - 53s 107ms/step - loss: 4.4771 - accuracy: 0.2578\n",
            "Epoch 6/50\n",
            "492/492 [==============================] - 53s 107ms/step - loss: 4.2702 - accuracy: 0.2697\n",
            "Epoch 7/50\n",
            "492/492 [==============================] - 53s 107ms/step - loss: 4.0818 - accuracy: 0.2809\n",
            "Epoch 8/50\n",
            "492/492 [==============================] - 53s 107ms/step - loss: 3.9115 - accuracy: 0.2945\n",
            "Epoch 9/50\n",
            "492/492 [==============================] - 53s 107ms/step - loss: 3.7591 - accuracy: 0.3093\n",
            "Epoch 10/50\n",
            "492/492 [==============================] - 53s 107ms/step - loss: 3.6261 - accuracy: 0.3234\n",
            "Epoch 11/50\n",
            "492/492 [==============================] - 53s 107ms/step - loss: 3.5085 - accuracy: 0.3371\n",
            "Epoch 12/50\n",
            "492/492 [==============================] - 53s 107ms/step - loss: 3.4016 - accuracy: 0.3491\n",
            "Epoch 13/50\n",
            "492/492 [==============================] - 53s 107ms/step - loss: 3.3032 - accuracy: 0.3613\n",
            "Epoch 14/50\n",
            "492/492 [==============================] - 53s 107ms/step - loss: 3.2146 - accuracy: 0.3729\n",
            "Epoch 15/50\n",
            "492/492 [==============================] - 52s 107ms/step - loss: 3.1316 - accuracy: 0.3840\n",
            "Epoch 16/50\n",
            "492/492 [==============================] - 52s 107ms/step - loss: 3.0536 - accuracy: 0.3953\n",
            "Epoch 17/50\n",
            "492/492 [==============================] - 52s 107ms/step - loss: 2.9794 - accuracy: 0.4067\n",
            "Epoch 18/50\n",
            "492/492 [==============================] - 52s 107ms/step - loss: 2.9101 - accuracy: 0.4176\n",
            "Epoch 19/50\n",
            "492/492 [==============================] - 53s 107ms/step - loss: 2.8422 - accuracy: 0.4281\n",
            "Epoch 20/50\n",
            "492/492 [==============================] - 52s 107ms/step - loss: 2.7803 - accuracy: 0.4388\n",
            "Epoch 21/50\n",
            "492/492 [==============================] - 52s 106ms/step - loss: 2.7201 - accuracy: 0.4489\n",
            "Epoch 22/50\n",
            "492/492 [==============================] - 53s 107ms/step - loss: 2.6622 - accuracy: 0.4588\n",
            "Epoch 23/50\n",
            "492/492 [==============================] - 52s 106ms/step - loss: 2.6064 - accuracy: 0.4680\n",
            "Epoch 24/50\n",
            "492/492 [==============================] - 53s 107ms/step - loss: 2.5500 - accuracy: 0.4788\n",
            "Epoch 25/50\n",
            "492/492 [==============================] - 53s 107ms/step - loss: 2.4976 - accuracy: 0.4883\n",
            "Epoch 26/50\n",
            "492/492 [==============================] - 53s 107ms/step - loss: 2.4477 - accuracy: 0.4977\n",
            "Epoch 27/50\n",
            "492/492 [==============================] - 53s 107ms/step - loss: 2.3966 - accuracy: 0.5070\n",
            "Epoch 28/50\n",
            "492/492 [==============================] - 53s 108ms/step - loss: 2.3536 - accuracy: 0.5150\n",
            "Epoch 29/50\n",
            "492/492 [==============================] - 53s 107ms/step - loss: 2.3021 - accuracy: 0.5246\n",
            "Epoch 30/50\n",
            "492/492 [==============================] - 53s 108ms/step - loss: 2.2594 - accuracy: 0.5330\n",
            "Epoch 31/50\n",
            "492/492 [==============================] - 53s 108ms/step - loss: 2.2181 - accuracy: 0.5418\n",
            "Epoch 32/50\n",
            "492/492 [==============================] - 53s 108ms/step - loss: 2.1733 - accuracy: 0.5505\n",
            "Epoch 33/50\n",
            "492/492 [==============================] - 53s 108ms/step - loss: 2.1331 - accuracy: 0.5587\n",
            "Epoch 34/50\n",
            "492/492 [==============================] - 53s 108ms/step - loss: 2.0950 - accuracy: 0.5663\n",
            "Epoch 35/50\n",
            "492/492 [==============================] - 53s 108ms/step - loss: 2.0595 - accuracy: 0.5731\n",
            "Epoch 36/50\n",
            "492/492 [==============================] - 53s 108ms/step - loss: 2.0278 - accuracy: 0.5801\n",
            "Epoch 37/50\n",
            "492/492 [==============================] - 53s 107ms/step - loss: 1.9968 - accuracy: 0.5862\n",
            "Epoch 38/50\n",
            "492/492 [==============================] - 53s 108ms/step - loss: 1.9609 - accuracy: 0.5932\n",
            "Epoch 39/50\n",
            "492/492 [==============================] - 53s 108ms/step - loss: 1.9274 - accuracy: 0.6011\n",
            "Epoch 40/50\n",
            "492/492 [==============================] - 53s 108ms/step - loss: 1.8955 - accuracy: 0.6082\n",
            "Epoch 41/50\n",
            "492/492 [==============================] - 53s 108ms/step - loss: 1.8641 - accuracy: 0.6145\n",
            "Epoch 42/50\n",
            "492/492 [==============================] - 53s 108ms/step - loss: 1.8353 - accuracy: 0.6212\n",
            "Epoch 43/50\n",
            "492/492 [==============================] - 53s 108ms/step - loss: 1.8065 - accuracy: 0.6277\n",
            "Epoch 44/50\n",
            "492/492 [==============================] - 53s 107ms/step - loss: 1.7763 - accuracy: 0.6340\n",
            "Epoch 45/50\n",
            "492/492 [==============================] - 53s 108ms/step - loss: 1.7444 - accuracy: 0.6408\n",
            "Epoch 46/50\n",
            "492/492 [==============================] - 53s 108ms/step - loss: 1.7195 - accuracy: 0.6468\n",
            "Epoch 47/50\n",
            "492/492 [==============================] - 53s 108ms/step - loss: 1.6889 - accuracy: 0.6538\n",
            "Epoch 48/50\n",
            "492/492 [==============================] - 53s 108ms/step - loss: 1.6626 - accuracy: 0.6602\n",
            "Epoch 49/50\n",
            "492/492 [==============================] - 53s 108ms/step - loss: 1.6357 - accuracy: 0.6659\n",
            "Epoch 50/50\n",
            "492/492 [==============================] - 53s 108ms/step - loss: 1.6140 - accuracy: 0.6711\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cTuSLBzTI0aV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "8865a6cc-bc27-4c53-f567-b6364fdcb784"
      },
      "source": [
        "main_lstm_model = create_model_lstm(vocab_size = len(vocab), embedding_dim=embedding_dim, rnn_units=rnn_units, batch_size=1)\n",
        "main_lstm_model.set_weights(lstm_model.get_weights())\n",
        "main_lstm_model.summary()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_4 (Embedding)      (1, None, 256)            5063680   \n",
            "_________________________________________________________________\n",
            "lstm_2 (LSTM)                (1, None, 1024)           5246976   \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (1, None, 19780)          20274500  \n",
            "=================================================================\n",
            "Total params: 30,585,156\n",
            "Trainable params: 30,585,156\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_tzcVlwoq1Qg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# main_lstm_model = tf.keras.models.load_model('word_lstm.h5')\n",
        "# main_lstm_model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dtcL1MWoI8zi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "32613bba-69fd-4405-9985-eca93aed31b1"
      },
      "source": [
        "print(generate_text(main_lstm_model, start_string=u\"که ایران چوباغیست خرم بهار\", temperature=0.5))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "که ایران چوباغیست خرم بهار\t ز خون برادر خروشان شدند \n",
            " همی گفت با لشکر و پهلوان \t هشیوار و بادانش و یادگیر \n",
            " بدو گفت پیران که ای پهلوان \t پدرت از پی گیو پیران ز راه \n",
            " ببستندشان دست و پای آهنست \t نه خرگاه بود \n",
            " چو آمد بنزدیک بر تخت خویش \t بخون غرقه گردانم این رزمگاه \n",
            " همه خستگان از پس یکدگر \t بفرمود تا زنگه شاوران \n",
            " بدان رزمگه شد فرستاده زود \t سپه کرد و جنگ از پس پشت اوی \t همی‌آمد از بوستان پرورش \n",
            " چو بر گردن تیز بگذاشتند \t همی آمد از دشت نخچیرگاه \n",
            " یکی نامه بنوشت خوب چهر \t چو باد دمان لشکر آنجا رسید \n",
            " چو از دیده گه دیده‌بانش بدید \t سران را ز لشکر بپای آورید \n",
            " ابا جوشن و تیغ و گرز و کمند \t همی رفت با نامور ده سوار \n",
            " بکردند پیمان و گشتند باز \t گرفتند کوتاه رزم دراز \n",
            " دو سالار و بر هر سویی جامه خواست \t چهل رش درم تنگ بسته کمر \n",
            " سواران رومی بپیش من آر \t که تا بگسلاند ز کینه دلی پر ز دود \n",
            " که از ما کسی نیست جفت \n",
            " برفتند یارانش را یکسره \t ببشخور\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m6D8yNrxKO9Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "main_lstm_model.save(\"word_lstm.h5\")"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
