{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tempo.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aliakbarbadri/persian-poetry-creator/blob/master/word-lstm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nF-7m57sBgdp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional, GRU\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras import regularizers\n",
        "import tensorflow.keras.utils as ku \n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Urefe0BUJ5vy",
        "colab_type": "text"
      },
      "source": [
        "usefull links:\n",
        "\n",
        "https://www.tensorflow.org/tutorials/text/text_generation\n",
        "\n",
        "https://github.com/petrosDemetrakopoulos/RNN-Beatles-lyrics-generator/blob/master/model.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pxSaPGaqHfMV",
        "colab_type": "text"
      },
      "source": [
        "# Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0oP6TLTmhdx5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def corpusToList(corpus):\n",
        "  corpusList = [w for w in corpus.split(' ')] \n",
        "  corpusList = [i for i in corpusList if i]\n",
        "  return corpusList"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NNCS1WcxmTMB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9f246dc2-1df4-4909-ab71-0f1d991c1821"
      },
      "source": [
        "url = \"https://raw.githubusercontent.com/aliakbarbadri/persian-poetry-creator/master/shahname2.txt\"\n",
        "filepath = keras.utils.get_file(\"shahname2.txt\", url)\n",
        "\n",
        "text = open(filepath, 'rb').read().decode(encoding='utf-8')\n",
        "text = text.replace(\"\\t\",\" \\t \").replace(\"\\n\", \" \\n \")\n",
        "corpusList = [w for w in text.split(' ')] \n",
        "corpus_words = [i for i in corpusList if i]\n",
        "# corpus_words = corpusToList(text) \n",
        "map(str.strip, corpus_words) #trim words"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<map at 0x7ff803b132e8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f11TwQxzmVBv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "490529cb-18fc-4829-b9c7-5800250153b6"
      },
      "source": [
        "vocab = sorted(set(corpus_words))\n",
        "print('Corpus length (in words):', len(corpus_words))\n",
        "print('Unique words in corpus: {}'.format(len(vocab)))\n",
        "word2idx = {u: i for i, u in enumerate(vocab)}\n",
        "idx2words = np.array(vocab)\n",
        "word_as_int = np.array([word2idx[c] for c in corpus_words])"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Corpus length (in words): 661330\n",
            "Unique words in corpus: 19780\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LBLvTwoNhep0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "seqLength = 20\n",
        "examples_per_epoch = len(corpus_words)//(seqLength + 1)\n",
        "wordDataset = tf.data.Dataset.from_tensor_slices(word_as_int)\n",
        "sequencesOfWords = wordDataset.batch(seqLength + 1, drop_remainder=True)\n",
        "def split_input_target(chunk):\n",
        "  input_text = chunk[:-1]\n",
        "  target_text = chunk[1:]\n",
        "  return input_text, target_text\n",
        "\n",
        "dataset = sequencesOfWords.map(split_input_target)\n",
        "BATCH_SIZE = 64\n",
        "BUFFER_SIZE = 100\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tmgeH1iaHV3D",
        "colab_type": "text"
      },
      "source": [
        "# Model (GRU)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rle0H24_GGoc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_model_gru(vocab_size, embedding_dim, rnn_units, batch_size):\n",
        "  model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
        "                              batch_input_shape=[batch_size, None]),\n",
        "    tf.keras.layers.GRU(rnn_units,\n",
        "                        return_sequences=True,\n",
        "                        stateful=True,\n",
        "                        recurrent_initializer='glorot_uniform'),\n",
        "    tf.keras.layers.Dense(vocab_size)\n",
        "  ])\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ypzdXYqhsqF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 935
        },
        "outputId": "d867e99a-281d-40ff-d5cd-a2af7d102b4c"
      },
      "source": [
        "vocab_size = len(vocab)\n",
        "embedding_dim = 256\n",
        "rnn_units = 1024\n",
        "\n",
        "gru_model = create_model_gru(vocab_size = len(vocab), embedding_dim=embedding_dim, rnn_units=rnn_units, batch_size=BATCH_SIZE)\n",
        "\n",
        "gru_model.summary()\n",
        "\n",
        "gru_model.compile(optimizer='adam', loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])\n",
        "history = gru_model.fit(dataset, epochs=20)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_4 (Embedding)      (64, None, 256)           5063680   \n",
            "_________________________________________________________________\n",
            "gru_4 (GRU)                  (64, None, 1024)          3938304   \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (64, None, 19780)         20274500  \n",
            "=================================================================\n",
            "Total params: 29,276,484\n",
            "Trainable params: 29,276,484\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n",
            "492/492 [==============================] - 46s 94ms/step - loss: 6.4823\n",
            "Epoch 2/20\n",
            "492/492 [==============================] - 46s 94ms/step - loss: 5.6372\n",
            "Epoch 3/20\n",
            "492/492 [==============================] - 46s 94ms/step - loss: 4.9428\n",
            "Epoch 4/20\n",
            "492/492 [==============================] - 46s 94ms/step - loss: 4.5214\n",
            "Epoch 5/20\n",
            "492/492 [==============================] - 46s 94ms/step - loss: 4.1558\n",
            "Epoch 6/20\n",
            "492/492 [==============================] - 46s 94ms/step - loss: 3.8164\n",
            "Epoch 7/20\n",
            "492/492 [==============================] - 46s 93ms/step - loss: 3.5165\n",
            "Epoch 8/20\n",
            "492/492 [==============================] - 46s 93ms/step - loss: 3.2576\n",
            "Epoch 9/20\n",
            "492/492 [==============================] - 46s 93ms/step - loss: 3.0358\n",
            "Epoch 10/20\n",
            "492/492 [==============================] - 45s 92ms/step - loss: 2.8446\n",
            "Epoch 11/20\n",
            "492/492 [==============================] - 46s 93ms/step - loss: 2.6795\n",
            "Epoch 12/20\n",
            "492/492 [==============================] - 45s 92ms/step - loss: 2.5395\n",
            "Epoch 13/20\n",
            "492/492 [==============================] - 46s 93ms/step - loss: 2.4140\n",
            "Epoch 14/20\n",
            "492/492 [==============================] - 46s 93ms/step - loss: 2.3067\n",
            "Epoch 15/20\n",
            "492/492 [==============================] - 46s 93ms/step - loss: 2.2137\n",
            "Epoch 16/20\n",
            "492/492 [==============================] - 46s 93ms/step - loss: 2.1292\n",
            "Epoch 17/20\n",
            "492/492 [==============================] - 45s 92ms/step - loss: 2.0563\n",
            "Epoch 18/20\n",
            "492/492 [==============================] - 46s 93ms/step - loss: 1.9915\n",
            "Epoch 19/20\n",
            "492/492 [==============================] - 46s 93ms/step - loss: 1.9329\n",
            "Epoch 20/20\n",
            "492/492 [==============================] - 46s 93ms/step - loss: 1.8782\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hxs6LcdkA-qC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "675b5056-2ca7-4d5a-8923-1a05322e1d4f"
      },
      "source": [
        "main_gru_model = create_model_gru(vocab_size = len(vocab), embedding_dim=embedding_dim, rnn_units=rnn_units, batch_size=1)\n",
        "main_gru_model.set_weights(gru_model.get_weights())\n",
        "main_gru_model.summary()"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_6\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_6 (Embedding)      (1, None, 256)            5063680   \n",
            "_________________________________________________________________\n",
            "gru_6 (GRU)                  (1, None, 1024)           3938304   \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (1, None, 19780)          20274500  \n",
            "=================================================================\n",
            "Total params: 29,276,484\n",
            "Trainable params: 29,276,484\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KL4gEU440DyQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_text(model, start_string, temperature = 1.0):\n",
        "  num_generate = 200\n",
        "  start_string_list =  [w for w in start_string.split(' ')]\n",
        "  input_eval = [word2idx[s] for s in start_string_list]\n",
        "  input_eval = tf.expand_dims(input_eval, 0)\n",
        "  text_generated = []\n",
        "  model.reset_states()\n",
        "  for i in range(num_generate):\n",
        "      predictions = model(input_eval)\n",
        "      predictions = tf.squeeze(predictions, 0)\n",
        "      predictions = predictions / temperature\n",
        "      predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
        "      input_eval = tf.expand_dims([predicted_id], 0)\n",
        "      text_generated.append(idx2words[predicted_id])\n",
        "  return (start_string + ' '.join(text_generated))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yAduGkXN0SYs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "outputId": "edd4b448-055d-41df-fa6b-016ad4762acb"
      },
      "source": [
        "print(generate_text(main_model, start_string=u\"که ایران چوباغیست خرم بهار\", temperature=1))"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "که ایران چوباغیست خرم بهار\t گهی مهربان بودی و گاهی کلاه \n",
            " چو آن دیده‌بانان ایران سپاه \t نگون اندر آمد ز پهلو براه \n",
            " چو هومان ازان جایگاه نبرد \t ز گودرز کمتر گریزان ز باد \n",
            " بنزدیک پیران و هومان گیو \t نشستند و گفتند کین رای دیگر نگهدار من \n",
            " چنین داد پاسخ ورا پیرمرد \t همیدون همیدون ز لشکر سوار \n",
            " ز لشکر سخنگوی مردی هزار \t سواران و شیران خنجرگزار \n",
            " بقلب اندر آمد سخن راندند \t همه جان یک بادگر درد را تنگ دارند کین خواستن \t بکینه میان دو رویه جگر \n",
            " تن خسته بودند گردان اسیر \t روان‌ها همه باد و یزدان‌پرست \n",
            " ز هر گونه بسیار و بنشاندش \t چو پروین شدش سوی ایشان هنر \n",
            " چو گودرز گودرز پیر و پیکار چیست \n",
            " ز بخشیدن شوم گر ترا \t گر اینست پیمان تو من نبرد \n",
            " پی سال دیدی ز کار سپاه \t فراز آوریدند پیلی همیدون بدست \n",
            " که گودرز را جز برین گونه فرمود شاه \t بفرمود کردن همه سربسر گرد کرد \n",
            " بجویید درباغ بگشاد بیژن پیام \t سخن چون بباید پرستار جنگ \n",
            " روان مسیحا پر از رنگ و بوی \t چنان خوش که رفتن ز بیچارگیست \t ز بهرام\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gEtcew15KI0O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.save(\"word_gru.h5\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OLOB0rSQHbe7",
        "colab_type": "text"
      },
      "source": [
        "# Model (LSTM)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "47qIyVAcHdak",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_model_lstm(vocab_size, embedding_dim, rnn_units, batch_size):\n",
        "  model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
        "                              batch_input_shape=[batch_size, None]),\n",
        "    tf.keras.layers.LSTM(rnn_units,\n",
        "                        return_sequences=True,\n",
        "                        stateful=True,\n",
        "                        recurrent_initializer='glorot_uniform'),\n",
        "    tf.keras.layers.Dense(vocab_size)\n",
        "  ])\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PLuzYVgrIyLv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 935
        },
        "outputId": "cd393572-82ac-4831-e22a-964a75fc050d"
      },
      "source": [
        "vocab_size = len(vocab)\n",
        "embedding_dim = 256\n",
        "rnn_units = 1024\n",
        "\n",
        "lstm_model = create_model_lstm(vocab_size = len(vocab), embedding_dim=embedding_dim, rnn_units=rnn_units, batch_size=BATCH_SIZE)\n",
        "lstm_model.summary()\n",
        "lstm_model.compile(optimizer='adam', loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])\n",
        "history = lstm_model.fit(dataset, epochs=20)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_7\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_7 (Embedding)      (64, None, 256)           5063680   \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (64, None, 1024)          5246976   \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (64, None, 19780)         20274500  \n",
            "=================================================================\n",
            "Total params: 30,585,156\n",
            "Trainable params: 30,585,156\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n",
            "492/492 [==============================] - 55s 111ms/step - loss: 6.1975 - accuracy: 0.1123\n",
            "Epoch 2/20\n",
            "492/492 [==============================] - 54s 110ms/step - loss: 5.4306 - accuracy: 0.1791\n",
            "Epoch 3/20\n",
            "492/492 [==============================] - 54s 111ms/step - loss: 5.0112 - accuracy: 0.2265\n",
            "Epoch 4/20\n",
            "492/492 [==============================] - 54s 111ms/step - loss: 4.7216 - accuracy: 0.2437\n",
            "Epoch 5/20\n",
            "492/492 [==============================] - 54s 110ms/step - loss: 4.4847 - accuracy: 0.2571\n",
            "Epoch 6/20\n",
            "492/492 [==============================] - 54s 111ms/step - loss: 4.2753 - accuracy: 0.2697\n",
            "Epoch 7/20\n",
            "492/492 [==============================] - 54s 111ms/step - loss: 4.0890 - accuracy: 0.2809\n",
            "Epoch 8/20\n",
            "492/492 [==============================] - 55s 111ms/step - loss: 3.9184 - accuracy: 0.2940\n",
            "Epoch 9/20\n",
            "492/492 [==============================] - 54s 111ms/step - loss: 3.7675 - accuracy: 0.3082\n",
            "Epoch 10/20\n",
            "492/492 [==============================] - 55s 111ms/step - loss: 3.6328 - accuracy: 0.3220\n",
            "Epoch 11/20\n",
            "492/492 [==============================] - 55s 111ms/step - loss: 3.5154 - accuracy: 0.3354\n",
            "Epoch 12/20\n",
            "492/492 [==============================] - 55s 112ms/step - loss: 3.4081 - accuracy: 0.3481\n",
            "Epoch 13/20\n",
            "492/492 [==============================] - 55s 112ms/step - loss: 3.3139 - accuracy: 0.3595\n",
            "Epoch 14/20\n",
            "492/492 [==============================] - 55s 112ms/step - loss: 3.2268 - accuracy: 0.3705\n",
            "Epoch 15/20\n",
            "492/492 [==============================] - 55s 112ms/step - loss: 3.1425 - accuracy: 0.3824\n",
            "Epoch 16/20\n",
            "492/492 [==============================] - 55s 111ms/step - loss: 3.0676 - accuracy: 0.3932\n",
            "Epoch 17/20\n",
            "492/492 [==============================] - 54s 111ms/step - loss: 2.9938 - accuracy: 0.4042\n",
            "Epoch 18/20\n",
            "492/492 [==============================] - 55s 111ms/step - loss: 2.9242 - accuracy: 0.4148\n",
            "Epoch 19/20\n",
            "492/492 [==============================] - 55s 111ms/step - loss: 2.8559 - accuracy: 0.4257\n",
            "Epoch 20/20\n",
            "492/492 [==============================] - 55s 111ms/step - loss: 2.7922 - accuracy: 0.4361\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cTuSLBzTI0aV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "26161ad2-0662-464a-94fb-f7020856ec55"
      },
      "source": [
        "main_lstm_model = create_model_lstm(vocab_size = len(vocab), embedding_dim=embedding_dim, rnn_units=rnn_units, batch_size=1)\n",
        "main_lstm_model.set_weights(lstm_model.get_weights())\n",
        "main_lstm_model.summary()"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_8\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_8 (Embedding)      (1, None, 256)            5063680   \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (1, None, 1024)           5246976   \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (1, None, 19780)          20274500  \n",
            "=================================================================\n",
            "Total params: 30,585,156\n",
            "Trainable params: 30,585,156\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dtcL1MWoI8zi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "outputId": "83c2bb42-34cf-4f07-ca14-9880f579c7fc"
      },
      "source": [
        "print(generate_text(main_lstm_model, start_string=u\"که ایران چوباغیست خرم بهار\", temperature=1))"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "که ایران چوباغیست خرم بهار\n",
            " فروتر سر از جای برخاستند \t بدیوان شاه از ایشان نکرد \n",
            " به پیلان برآمد سپه برگرفت \t ازیشان سخنها فراوان دلیر \n",
            " ازان پس رستم رای‌زن هرک آمد سپاه \t از آن روزبانان مردم‌کشان \t که ای شهریار جهان سربسر \n",
            " کجا سرو کشمرش گیرد بجنگ \t بیامد بدلش اندر آمد چو نیل \n",
            " دو دست از پس پشت بستش چو کوه \t بپیران نیارست شد ترجمان \n",
            " ببالا برآمد بروی نبرد \t شب و روز با تن شدند انجمن \n",
            " بدو بر ما یکی نامدار \t دلیر و پیاده شدش نیو را \n",
            " ابا ده هزار آزموده سران \t هشیوار و کردند تن سربسر هدیه گشت \n",
            " سراسر همه جامه و سیم \t پر از کینه و زرمساز آمدند \n",
            " ببارید خون دو رخ لاجورد \t هم آنگه بغلتید بر خیره روی \n",
            " چپ و بوم هندوستان \t گروگان بدست تو دست آورد \n",
            " سرش را بفتراک شبرنگ بست \n",
            " بشد بیژن گیو تا بود و بس \t خروشان ازیشان و مردان نیو \n",
            " جدا زیر پیلان و خود برنشست \t بدل پر ز کینه دلی پر ز گرد \n",
            " برآمد برین کینه درویش نیز \t ز پیکارشان دل تو بیگانه نیست \n",
            " یکی جنگ با پیل\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m6D8yNrxKO9Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.save(\"word_lstm.h5\")"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}